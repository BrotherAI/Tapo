Official implementation of the paper:  
**"TAPO: Dynamic Teacher and Perturbed Answer Injection for Policy Optimization"**  
ğŸš€ NeurIPS 2025 Submission Â· Anonymous Authors

TAPO is a fine-grained reinforcement learning framework that improves reasoning alignment in large language models (LLMs). Built on GRPO, TAPO introduces four innovations to inject token-level learning signals and stabilize optimization:

- âœ… **Dynamic Teacher Injection (DTI)**: Injects contrastive completions (correct/incorrect) into all-correct or all-wrong groups to recover collapsed gradients.
- âœ… **Perturbed Answer Injection (PAI)**: Flips answers of correct completions to create informative contrast and encourage robust reasoning.
- âœ… **Advantage-Level Intervention (ALI)**: Assigns fixed token-level advantages while preserving group normalization.
- âœ… **InfoLen-Aware Reward Shaping**: Penalizes overly long and redundant completions while preserving informativeness.

TAPO outperforms existing RLHF approaches (e.g., GRPO, DAPO) on math reasoning benchmarks such as AIME, MATH500, AMC, and Minerva-Math.

---

## ğŸ”§ Installation

```bash
git clone https://github.com/your-org/tapo.git
cd tapo
conda create -n tapo python=3.10
conda activate tapo
pip install -r requirements.txt
````

> TAPO is built on [VeRL](https://github.com/your-org/verl) and tested with PyTorch 2.1, Transformers 4.39, and 4x NVIDIA 4090 GPUs.

---

## ğŸ§  Training

To train TAPO from a pretrained policy (e.g., DeepSeek-R1-Distill-Qwen-1.5B):

```bash
python train.py \
  --config configs/tapo_math.yaml \
  --output_dir ./checkpoints/tapo_math \
  --use_dti --use_pai --use_infolen
```

You can ablate any module by removing the corresponding flag. Example: `--no_infolen`.

---

## ğŸ“Š Evaluation

Evaluate the trained model using pass\@1 accuracy on AIME / MATH / AMC:

```bash
python eval.py \
  --model_path ./checkpoints/tapo_math \
  --dataset aime2024
```

Evaluation results will be saved to `results/`.

---

## ğŸ“ Project Structure

```
tapo/
â”œâ”€â”€ core/                 # TAPO training components (DTI, PAI, ALI, InfoLen)
â”œâ”€â”€ configs/              # YAML configs for different experiments
â”œâ”€â”€ data/                 # Dataset loading and preprocessing
â”œâ”€â”€ models/               # Model wrappers (Qwen, DeepSeek, etc.)
â”œâ”€â”€ train.py              # Training entry
â”œâ”€â”€ eval.py               # Evaluation script
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Results (AIME 2024)

| Model         | AIME Pass\@1 (%) |
| ------------- | ---------------- |
| DeepSeek-Base | 28.9             |
| + Naive GRPO  | 34.3             |
| + TAPO (Full) | **44.3**         |

---

## ğŸ“– Citation

```bibtex
@article{anonymous2025tapo,
  title   = {TAPO: Dynamic Teacher and Perturbed Answer Injection for Policy Optimization},
  author  = {Anonymous Authors},
  journal = {Conference on Neural Information Processing Systems (NeurIPS)},
  year    = {2025}
}
```

---

## ğŸ¤ Acknowledgements

TAPO builds on the ideas of GRPO, DAPO, and VeRL. We thank the authors of DeepSeekMath and OpenAI-o1 for inspiring this line of work.

---

## ğŸ“¬ Contact

For questions or collaborations, please open an issue or submit a pull request.

```

---

å¦‚ä½ æƒ³ç”¨éåŒ¿åç‰ˆï¼ˆæ¯”å¦‚å¼€æºåï¼‰ï¼Œæˆ‘ä¹Ÿå¯ä»¥å¸®ä½ åŠ ä¸Šä½œè€…ã€å®éªŒæœºæ„ç­‰ã€‚å¦‚æœæœªæ¥æ¨¡å‹è®­ç»ƒä»£ç /æ•°æ®ä¾èµ–å˜æ›´ï¼Œä¹Ÿå¯ä»¥å¸®ä½ å†™ `setup.py` æˆ– `docs/` æ–‡æ¡£ç»“æ„ã€‚æ˜¯å¦éœ€è¦æˆ‘ä¹Ÿåšä¸€ç‰ˆâ€œä¸­è‹±æ–‡å¯¹ç…§ç‰ˆâ€READMEï¼Ÿ
```
